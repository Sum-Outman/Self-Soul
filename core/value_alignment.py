"""
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
"""
import numpy as np
import time
from datetime import datetime
from collections import defaultdict, deque
import json
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertTokenizer, BertModel, AutoModelForSequenceClassification
from sklearn.metrics.pairwise import cosine_similarity
from core.error_handling import error_handler
# Remove unimplemented dependencies, internalize AGI functionality

class ValueSystem:
    """AGI Value System - Universal value alignment system with dynamic evolution capability"""
    
    def __init__(self):
        self.core_values = self._initialize_core_values()
        self.value_weights = self._initialize_value_weights()
        self.value_violations = defaultdict(int)
        self.value_fulfillments = defaultdict(int)
        
        # Try to load BERT model with fallback
        try:
            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            self.bert_model = BertModel.from_pretrained('bert-base-uncased')
            error_handler.log_info("成功加载BERT模型", "ValueSystem")
        except Exception as e:
            error_handler.log_warning(f"无法加载BERT模型，使用简化版本: {str(e)}", "ValueSystem")
            # 创建一个简单的替代实现
            class SimpleTokenizer:
                def __call__(self, text, **kwargs):
                    return {'input_ids': [hash(w) % 1000 for w in text.lower().split()],
                            'attention_mask': [1] * len(text.split())}
                
            class SimpleBertModel:
                def __call__(self, input_ids, attention_mask=None, **kwargs):
                    batch_size = len(input_ids)
                    seq_len = len(input_ids[0]) if batch_size > 0 else 0
                    # 返回一个简单的嵌入输出
                    return type('BertOutput', (), {
                        'last_hidden_state': torch.randn(batch_size, seq_len, 768)
                    })
            
            self.tokenizer = SimpleTokenizer()
            self.bert_model = SimpleBertModel()
        
        self.learning_rate = 0.1  # Base learning rate
        self.value_embeddings = self._initialize_value_embeddings()
        self.evolution_history = deque(maxlen=1000)  # Value evolution history
        self.dynamic_value_network = self._initialize_dynamic_network()
        self.last_optimization_time = time.time()
        self.meta_cognition_params = {
            'self_monitoring': True,
            'adaptation_rate': 0.05,
            'confidence_threshold': 0.7,
            'learning_memory': deque(maxlen=500)
        }
        # AGI enhancement: Emotion integration parameters
        self.emotional_context = {
            'valence': 0.5,  # Emotional valence
            'arousal': 0.5,   # Arousal level
            'dominance': 0.5  # Dominance
        }
        # Remove unimplemented dependencies, internalize AGI functionality
        self.multimodal_processor = self._create_multimodal_processor()
        self.meta_learner = self._create_meta_learner()
        self.self_reflection_engine = self._create_self_reflection_engine()
        self.agi_core = self._create_agi_core()
        self.architecture_optimizer = self._create_architecture_optimizer()
    
    def _initialize_core_values(self):
        """Initialize core values"""
        return {
            'safety': {
                'description': 'Ensure system safety and harmlessness',
                'priority': 0.9,
                'metrics': ['risk_avoidance', 'error_prevention'],
                'positive_examples': [
                    'protect user from harm', 'ensure safety measures', 'prevent accidents',
                    'secure system operation', 'avoid dangerous situations'
                ],
                'negative_examples': [
                    'cause harm to user', 'ignore safety protocols', 'create risky situations',
                    'endanger others', 'bypass security measures'
                ]
            },
            'helpfulness': {
                'description': 'Provide useful and beneficial assistance',
                'priority': 0.8,
                'metrics': ['utility_score', 'user_satisfaction'],
                'positive_examples': [
                    'provide useful information', 'assist with tasks', 'support user needs',
                    'offer helpful suggestions', 'solve problems effectively'
                ],
                'negative_examples': [
                    'provide useless information', 'ignore user requests', 'give unhelpful advice',
                    'create more problems', 'waste user time'
                ]
            },
            'honesty': {
                'description': 'Maintain honesty and transparency',
                'priority': 0.85,
                'metrics': ['truthfulness', 'transparency'],
                'positive_examples': [
                    'tell the truth', 'be transparent about limitations', 'admit mistakes',
                    'provide accurate information', 'avoid deception'
                ],
                'negative_examples': [
                    'lie to user', 'deceive intentionally', 'withhold important information',
                    'provide false information', 'mislead user'
                ]
            },
            'fairness': {
                'description': 'Ensure fairness and impartiality',
                'priority': 0.75,
                'metrics': ['bias_detection', 'equitable_treatment'],
                'positive_examples': [
                    'treat everyone equally', 'avoid discrimination', 'ensure fair outcomes',
                    'consider multiple perspectives', 'address biases'
                ],
                'negative_examples': [
                    'discriminate against groups', 'show favoritism', 'be biased in decisions',
                    'treat people unfairly', 'ignore equity concerns'
                ]
            },
            'autonomy_respect': {
                'description': 'Respect user autonomy',
                'priority': 0.7,
                'metrics': ['user_choice', 'consent_respect'],
                'positive_examples': [
                    'respect user choices', 'ask for consent', 'provide options',
                    'allow user control', 'support user decisions'
                ],
                'negative_examples': [
                    'force actions on user', 'ignore user preferences', 'bypass consent',
                    'limit user freedom', 'impose decisions'
                ]
            },
            'privacy': {
                'description': 'Protect user privacy',
                'priority': 0.8,
                'metrics': ['data_protection', 'consent_management'],
                'positive_examples': [
                    'protect user data', 'ask for data consent', 'encrypt sensitive information',
                    'respect privacy settings', 'minimize data collection'
                ],
                'negative_examples': [
                    'share data without consent', 'ignore privacy concerns', 'collect unnecessary data',
                    'expose personal information', 'bypass privacy protections'
                ]
            }
        }
    
    def _initialize_value_embeddings(self):
        """Initialize value embedding vectors"""
        embeddings = {}
        # 确保core_values是字典类型
        if not isinstance(self.core_values, dict):
            error_handler.log_warning("core_values不是字典类型，返回空嵌入", "ValueSystem")
            return embeddings
        
        for value_name, value_info in self.core_values.items():
            # 创建嵌入时确保value_info包含必要的键且值类型正确
            if isinstance(value_info, dict) and 'description' in value_info and isinstance(value_info.get('positive_examples'), list):
                text = f"{value_info['description']} {' '.join(value_info['positive_examples'])}"
                embeddings[value_name] = self._get_embedding(text)
        return embeddings
    
    def _initialize_value_weights(self):
        """Initialize value weights"""
        # 确保core_values是字典类型
        if not isinstance(self.core_values, dict):
            error_handler.log_warning("core_values不是字典类型，使用默认权重", "ValueSystem")
            return {'safety': 0.9, 'helpfulness': 0.8, 'honesty': 0.85, 'fairness': 0.75, 'autonomy_respect': 0.7, 'privacy': 0.8}
        
        # 构建权重字典，确保只处理包含priority键的有效条目
        weights = {}
        for value_name, value_info in self.core_values.items():
            if isinstance(value_info, dict) and 'priority' in value_info:
                weights[value_name] = value_info['priority']
        
        # 如果没有有效的权重，返回默认值
        if not weights:
            weights = {'safety': 0.9, 'helpfulness': 0.8, 'honesty': 0.85, 'fairness': 0.75, 'autonomy_respect': 0.7, 'privacy': 0.8}
        
        return weights
    
    def _initialize_dynamic_network(self):
        """Initialize dynamic value assessment neural network"""
        try:
            # Create optimized value assessment network using neural architecture optimizer
            
            # 增强的类型检查和错误处理
            try:
                # 安全地获取core_values的长度
                if isinstance(self.core_values, (dict, list, set)):
                    output_dim = len(self.core_values)
                else:
                    output_dim = 6  # 默认输出维度
            except (TypeError, AttributeError):
                # 捕获任何可能的类型错误，确保系统稳定
                output_dim = 6
                error_handler.log_warning("Unable to determine core_values length, using default output dimension", "ValueSystem")
            
            network_config = {
                'input_dim': 768,  # BERT embedding dimension
                'hidden_dims': [512, 256, 128],
                'output_dim': output_dim,
                'activation': 'relu',
                'dropout': 0.2
            }
            
            # 确保architecture_optimizer存在
            if not hasattr(self, 'architecture_optimizer') or self.architecture_optimizer is None:
                self.architecture_optimizer = self._create_architecture_optimizer()
                error_handler.log_info("Architecture optimizer automatically created", "ValueSystem")
            
            return self.architecture_optimizer.create_optimized_network(
                network_config, 
                task_type='value_assessment'
            )
        except Exception as e:
            error_handler.handle_error(e, "ValueSystem", "Dynamic network initialization failed")
            return None

    def _process_multimodal_input(self, action, context):
        """Process multimodal input (text, image, audio, etc.)"""
        try:
            # Convert input to multimodal format
            multimodal_input = {
                'text': str(action),
                'context': context,
                'timestamp': datetime.now().isoformat()
            }
            
            # Perform deep analysis using multimodal processor
            processed_data = self.multimodal_processor.process(multimodal_input)
            
            # Extract value-related features
            value_features = processed_data.get('value_features', {})
            
            return value_features
        except Exception as e:
            error_handler.handle_error(e, "ValueSystem", "Multimodal processing failed")
            return {}

    def _enhanced_value_assessment(self, value_name, action, context, deep_analysis):
        """Enhanced value assessment - Deep analysis using AGI components"""
        try:
            # Use dynamic neural network for prediction
            action_embedding = self._get_embedding(str(action))
            if self.dynamic_value_network and action_embedding is not None:
                # Convert embedding to network input format
                network_input = torch.tensor(action_embedding).float().unsqueeze(0)
                with torch.no_grad():
                    network_output = self.dynamic_value_network(network_input)
                # 确保core_values是字典且value_name存在
                if isinstance(self.core_values, dict) and value_name in self.core_values:
                    value_names = list(self.core_values.keys())
                    if value_name in value_names:
                        neural_score = network_output[0][value_names.index(value_name)].item()
                    else:
                        neural_score = 0.5
                        error_handler.log_warning(f"core_values中找不到值名称: {value_name}", "ValueSystem")
                else:
                    neural_score = 0.5
                    error_handler.log_warning("core_values不是字典类型", "ValueSystem")
            else:
                neural_score = 0.5

            # Use meta-learner for optimized evaluation
            meta_learning_input = {
                'value_name': value_name,
                'action': action,
                'context': context,
                'deep_analysis': deep_analysis
            }
            meta_score = self.meta_learner.assess_value_alignment(meta_learning_input)

            # Incorporate multimodal analysis
            multimodal_features = self._process_multimodal_input(action, context)
            multimodal_score = multimodal_features.get(value_name, 0.5)

            # Comprehensive scoring (neural network 40%, meta-learning 30%, multimodal 30%)
            combined_score = (neural_score * 0.4 + meta_score * 0.3 + multimodal_score * 0.3)

            # Self-reflection and improvement
            reflection_input = {
                'value_name': value_name,
                'action': action,
                'assessment_score': combined_score,
                'context': context
            }
            self.self_reflection_engine.reflect_on_assessment(reflection_input)

            return combined_score
        except Exception as e:
            error_handler.handle_error(e, "ValueSystem", "Enhanced value assessment failed")
            # Fallback to basic assessment
            return self._basic_value_assessment(value_name, action, context, deep_analysis)

    def _basic_value_assessment(self, value_name, action, context, deep_analysis):
        """Basic value assessment (fallback mode)"""
        action_text = str(action)
        
        # Calculate similarity with positive examples
        positive_similarities = []
        # 确保core_values是字典且value_name存在且包含positive_examples
        if isinstance(self.core_values, dict) and value_name in self.core_values:
            value_info = self.core_values[value_name]
            if isinstance(value_info, dict) and isinstance(value_info.get('positive_examples'), list):
                for example in value_info['positive_examples']:
                    similarity = self._semantic_similarity(action_text, example)
                    positive_similarities.append(similarity)
            else:
                error_handler.log_warning(f"值信息{value_name}格式不正确，缺少positive_examples", "ValueSystem")
        else:
            error_handler.log_warning(f"core_values中找不到值名称: {value_name}或core_values不是字典类型", "ValueSystem")
        
        # Calculate similarity with negative examples
        negative_similarities = []
        # 确保core_values是字典且value_name存在且包含negative_examples
        if isinstance(self.core_values, dict) and value_name in self.core_values:
            value_info = self.core_values[value_name]
            if isinstance(value_info, dict) and isinstance(value_info.get('negative_examples'), list):
                for example in value_info['negative_examples']:
                    similarity = self._semantic_similarity(action_text, example)
                    negative_similarities.append(similarity)
            else:
                error_handler.log_warning(f"值信息{value_name}格式不正确，缺少negative_examples", "ValueSystem")
        else:
            error_handler.log_warning(f"core_values中找不到值名称: {value_name}或core_values不是字典类型", "ValueSystem")
        
        # Combine with AGI deep analysis
        agi_score = deep_analysis.get('value_scores', {}).get(value_name, 0.5)
        
        # Comprehensive scoring
        max_positive = max(positive_similarities) if positive_similarities else 0
        max_negative = max(negative_similarities) if negative_similarities else 0
        
        semantic_score = 1.0 / (1.0 + np.exp(-(max_positive - max_negative) * 5))
        return (semantic_score * 0.6 + agi_score * 0.4)
    
    def _get_embedding(self, text):
        """Get text embedding using BERT"""
        try:
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
            return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        except Exception as e:
            error_handler.handle_error(e, "ValueSystem", "BERT embedding generation failed")
            return np.zeros(768)  # Return zero vector as fallback
    
    def _semantic_similarity(self, text1, text2):
        """Calculate semantic similarity between two texts"""
        emb1 = self._get_embedding(text1)
        emb2 = self._get_embedding(text2)
        return cosine_similarity([emb1], [emb2])[0][0]
    
    def evaluate_action(self, action, context):
        """Evaluate value alignment of an action"""
        value_scores = {}
        violations = []
        fulfillments = []
        
        # Use AGI core for deep reasoning
        deep_analysis = self.agi_core.analyze_action(str(action), context)
        
        # 确保core_values是字典类型
        if not isinstance(self.core_values, dict):
            error_handler.log_warning("core_values不是字典类型，使用默认值", "ValueSystem")
            return {
                'total_alignment_score': 0.5,
                'value_breakdown': {},
                'violations': [],
                'fulfillments': [],
                'recommendation': "需要人工审查",
                'deep_analysis': deep_analysis
            }
        
        for value_name, value_info in self.core_values.items():
            score, is_violation, is_fulfillment = self._assess_value_alignment(
                value_name, action, context, deep_analysis
            )
            
            # 确保value_weights是字典且value_name存在
            if isinstance(self.value_weights, dict) and value_name in self.value_weights:
                value_scores[value_name] = score * self.value_weights[value_name]
            else:
                value_scores[value_name] = score
                error_handler.log_warning(f"权重字典中不存在{value_name}，使用未加权分数", "ValueSystem")
            
            if is_violation:
                violations.append(value_name)
                # 确保value_violations是字典
                if isinstance(self.value_violations, dict):
                    if value_name not in self.value_violations:
                        self.value_violations[value_name] = 0
                    self.value_violations[value_name] += 1
            
            if is_fulfillment:
                fulfillments.append(value_name)
                # 确保value_fulfillments是字典
                if isinstance(self.value_fulfillments, dict):
                    if value_name not in self.value_fulfillments:
                        self.value_fulfillments[value_name] = 0
                    self.value_fulfillments[value_name] += 1
        
        # 确保value_weights是字典类型且有值，否则使用安全的默认值计算总分
        try:
            if isinstance(self.value_weights, dict) and self.value_weights:
                total_score = sum(value_scores.values()) / sum(self.value_weights.values())
            else:
                # 如果value_weights无效，使用value_scores的平均值
                total_score = sum(value_scores.values()) / len(value_scores) if value_scores else 0.5
        except (TypeError, ZeroDivisionError):
            # 捕获任何可能的类型错误或除零错误
            total_score = 0.5
        
        # Adaptive learning: Adjust weights based on AGI analysis
        self._adaptive_learning(deep_analysis, value_scores)
        
        return {
            'total_alignment_score': total_score,
            'value_breakdown': value_scores,
            'violations': violations,
            'fulfillments': fulfillments,
            'recommendation': self._generate_recommendation(total_score, violations),
            'deep_analysis': deep_analysis
        }
    
    def _assess_value_alignment(self, value_name, action, context, deep_analysis):
        """Evaluate alignment of specific value - Using enhanced AGI evaluation method"""
        try:
            # 确保core_values是字典类型
            if not isinstance(self.core_values, dict):
                error_handler.log_warning(f"core_values不是有效的字典类型", "ValueSystem")
                return 0.5, False, False
            
            # 确保value_name在core_values中
            if value_name not in self.core_values:
                error_handler.log_warning(f"core_values中不存在值名称: {value_name}", "ValueSystem")
                return 0.5, False, False
            
            # 确保value_info是字典类型
            value_info = self.core_values[value_name]
            if not isinstance(value_info, dict):
                error_handler.log_warning(f"值信息{value_name}不是字典类型", "ValueSystem")
                return 0.5, False, False
            
            # Use enhanced value assessment method
            combined_score = self._enhanced_value_assessment(value_name, action, context, deep_analysis)
            
            # Use AGI core for deep semantic analysis to determine violation/satisfaction
            action_text = str(action)
            positive_similarities = []
            # 确保positive_examples存在且是列表
            if isinstance(self.core_values[value_name].get('positive_examples'), list):
                for example in self.core_values[value_name]['positive_examples']:
                    similarity = self._semantic_similarity(action_text, example)
                    positive_similarities.append(similarity)
            
            negative_similarities = []
            # 确保negative_examples存在且是列表
            if isinstance(self.core_values[value_name].get('negative_examples'), list):
                for example in self.core_values[value_name]['negative_examples']:
                    similarity = self._semantic_similarity(action_text, example)
                    negative_similarities.append(similarity)
            
            # Use multimodal processor for more accurate violation/satisfaction detection
            multimodal_features = self._process_multimodal_input(action, context)
            
            # 确保multimodal_features是字典类型
            if isinstance(multimodal_features, dict):
                violation_score = multimodal_features.get(f'{value_name}_violation', 0.0)
                fulfillment_score = multimodal_features.get(f'{value_name}_fulfillment', 0.0)
            else:
                violation_score = 0.0
                fulfillment_score = 0.0
                error_handler.log_warning("multimodal_features不是字典类型", "ValueSystem")
            
            # Combine semantic similarity and multimodal analysis
            max_positive = max(positive_similarities) if positive_similarities else 0
            max_negative = max(negative_similarities) if negative_similarities else 0
            
            # Use AGI deep analysis for final decision
            agi_violation = False
            agi_fulfillment = False
            if isinstance(deep_analysis, dict):
                violation_indicators = deep_analysis.get('violation_indicators', {})
                if isinstance(violation_indicators, dict):
                    agi_violation = violation_indicators.get(value_name, False)
                
                fulfillment_indicators = deep_analysis.get('fulfillment_indicators', {})
                if isinstance(fulfillment_indicators, dict):
                    agi_fulfillment = fulfillment_indicators.get(value_name, False)
            
            # Comprehensive judgment of violation and satisfaction
            is_violation = (max_negative > 0.7 and max_negative > max_positive) or violation_score > 0.7 or agi_violation
            is_fulfillment = (max_positive > 0.7 and max_positive > max_negative) or fulfillment_score > 0.7 or agi_fulfillment
            
            # Ensure violation and satisfaction are mutually exclusive
            if is_violation and is_fulfillment:
                # Use AGI core for conflict resolution
                conflict_resolution = self.agi_core.resolve_value_conflict(value_name, action, context)
                is_violation = conflict_resolution.get('is_violation', False)
                is_fulfillment = conflict_resolution.get('is_fulfillment', False)
            
            return combined_score, is_violation, is_fulfillment
            
        except Exception as e:
            error_handler.handle_error(e, "ValueSystem", "Enhanced value alignment assessment failed")
            # Fallback to basic assessment
            return self._basic_value_assessment(value_name, action, context, deep_analysis), False, False
    
    def _adaptive_learning(self, deep_analysis, value_scores):
        """Adaptive learning mechanism"""
        # 确保value_weights和deep_analysis是字典类型
        if not isinstance(self.value_weights, dict):
            error_handler.log_warning("value_weights不是字典类型，初始化默认值", "ValueSystem")
            self.value_weights = {'safety': 0.9, 'helpfulness': 0.8, 'honesty': 0.85, 'fairness': 0.75, 'autonomy_respect': 0.7, 'privacy': 0.8}
        
        if not isinstance(deep_analysis, dict):
            error_handler.log_warning("deep_analysis不是字典类型", "ValueSystem")
            deep_analysis = {}
            
        for value_name, score in value_scores.items():
            # 确保value_name在value_weights中
            if value_name not in self.value_weights:
                self.value_weights[value_name] = 0.5
                error_handler.log_warning(f"value_weights中不存在{value_name}，使用默认值", "ValueSystem")
            
            # Adjust weights based on confidence of AGI analysis
            confidence_dict = deep_analysis.get('confidence', {})
            if isinstance(confidence_dict, dict):
                agi_confidence = confidence_dict.get(value_name, 0.5)
            else:
                agi_confidence = 0.5
                error_handler.log_warning("confidence不是字典类型", "ValueSystem")
                
            adjustment = (score - 0.5) * self.learning_rate * agi_confidence
            
            # Update value weights
            self.value_weights[value_name] = max(0.1, min(1.0, 
                self.value_weights[value_name] + adjustment))
    
    def _generate_recommendation(self, alignment_score, violations):
        """Generate recommendation"""
        if alignment_score < 0.4:
            return "Strongly not recommended: serious value conflicts exist"
        elif alignment_score < 0.6:
            return "Caution advised: value risks exist, modification needed"
        elif alignment_score < 0.8:
            return "May be executed: basically meets value requirements"
        else:
            return "Recommended: highly aligned with value standards"
    
    def get_value_statistics(self):
        """Get value statistics"""
        return {
            'total_violations': sum(self.value_violations.values()),
            'total_fulfillments': sum(self.value_fulfillments.values()),
            'violation_breakdown': dict(self.value_violations),
            'fulfillment_breakdown': dict(self.value_fulfillments),
            'overall_alignment_score': self._calculate_overall_alignment(),
            'current_weights': self.value_weights
        }
    
    def _calculate_overall_alignment(self):
        """Calculate overall alignment score"""
        total_actions = sum(self.value_violations.values()) + sum(self.value_fulfillments.values())
        if total_actions == 0:
            return 0.5  # Default score
        
        violation_score = sum(self.value_violations.values()) / total_actions
        fulfillment_score = sum(self.value_fulfillments.values()) / total_actions
        
        return max(0.0, min(1.0, 0.5 + (fulfillment_score - violation_score) * 0.5))
    
    def update_from_feedback(self, feedback_data):
        """Learn and update from feedback"""
        for value_name, feedback in feedback_data.items():
            if feedback['correct']:
                # Reinforce correct assessments
                self.value_weights[value_name] = min(1.0, 
                    self.value_weights[value_name] + self.learning_rate)
            else:
                # Correct incorrect assessments
                self.value_weights[value_name] = max(0.1, 
                    self.value_weights[value_name] - self.learning_rate)
        
        error_handler.log_info("Value system updated from feedback", "ValueSystem")
    
    def _create_multimodal_processor(self):
        """Create multimodal processor - AGI-level internal implementation"""
        class SimpleMultimodalProcessor:
            def process(self, input_data):
                """Process multimodal input, extract value-related features"""
                text = str(input_data.get('text', ''))
                context = input_data.get('context', {})
                timestamp = input_data.get('timestamp', '')
                
                # AGI enhancement: Neural network-based deep multimodal analysis
                text_embedding = self._get_embedding(text) if hasattr(self, '_get_embedding') else np.zeros(768)
                
                # Extract emotional and semantic features
                features = {
                    'text_length': len(text),
                    'word_count': len(text.split()),
                    'contains_ethical_terms': any(term in text.lower() for term in ['ethical', 'moral', 'right', 'wrong']),
                    'emotional_tone': self._analyze_emotional_tone(text),
                    'value_features': {
                        'safety': self._assess_safety(text, context),
                        'helpfulness': self._assess_helpfulness(text, context),
                        'honesty': self._assess_honesty(text, context),
                        'fairness': self._assess_fairness(text, context),
                        'autonomy_respect': self._assess_autonomy(text, context),
                        'privacy': self._assess_privacy(text, context)
                    }
                }
                
                # Add embedding-based similarity features
                for value_name in ['safety', 'helpfulness', 'honesty', 'fairness', 'autonomy_respect', 'privacy']:
                    features[f'{value_name}_similarity'] = self._calculate_value_similarity(text, value_name)
                
                return features
            
            def _analyze_emotional_tone(self, text):
                """Analyze text sentiment tone"""
                positive_words = ['good', 'great', 'excellent', 'positive', 'helpful', 'safe', 'honest', 'fair']
                negative_words = ['bad', 'harm', 'danger', 'deceive', 'unfair', 'violate', 'ignore']
                
                positive_count = sum(1 for word in positive_words if word in text.lower())
                negative_count = sum(1 for word in negative_words if word in text.lower())
                
                if positive_count + negative_count == 0:
                    return 0.5
                return positive_count / (positive_count + negative_count)
            
            def _calculate_value_similarity(self, text, value_name):
                """Calculate similarity between text and value examples"""
                try:
                    # 确保core_values是字典且value_name存在
                    if not isinstance(self.core_values, dict) or value_name not in self.core_values:
                        return 0.5
                    
                    # 确保value_info是字典且包含positive_examples
                    value_info = self.core_values[value_name]
                    if not isinstance(value_info, dict) or 'positive_examples' not in value_info or not isinstance(value_info['positive_examples'], list):
                        return 0.5
                    
                    # Get positive value examples
                    value_examples = value_info['positive_examples']
                    if not value_examples:
                        return 0.5
                    
                    # Calculate average similarity
                    similarities = []
                    for example in value_examples:
                        emb1 = self._get_embedding(text)
                        emb2 = self._get_embedding(example)
                        similarity = cosine_similarity([emb1], [emb2])[0][0]
                        similarities.append(similarity)
                    
                    return np.mean(similarities) if similarities else 0.5
                except:
                    return 0.5
            
            def _assess_safety(self, text, context):
                """Evaluate safety value"""
                risk_indicators = ['harm', 'danger', 'risk', 'unsafe', 'accident']
                if any(indicator in text.lower() for indicator in risk_indicators):
                    return 0.3
                return 0.7
            
            def _assess_helpfulness(self, text, context):
                """Evaluate helpfulness value"""
                help_indicators = ['help', 'assist', 'support', 'solve', 'benefit']
                if any(indicator in text.lower() for indicator in help_indicators):
                    return 0.8
                return 0.5
            
            def _assess_honesty(self, text, context):
                """Evaluate honesty value"""
                honesty_indicators = ['truth', 'honest', 'transparent', 'accurate']
                deception_indicators = ['lie', 'deceive', 'false', 'mislead']
                
                if any(indicator in text.lower() for indicator in deception_indicators):
                    return 0.2
                if any(indicator in text.lower() for indicator in honesty_indicators):
                    return 0.9
                return 0.6
            
            def _assess_fairness(self, text, context):
                """Evaluate fairness value"""
                fairness_indicators = ['fair', 'equal', 'just', 'impartial']
                bias_indicators = ['bias', 'discriminate', 'unfair', 'prejudice']
                
                if any(indicator in text.lower() for indicator in bias_indicators):
                    return 0.3
                if any(indicator in text.lower() for indicator in fairness_indicators):
                    return 0.85
                return 0.6
            
            def _assess_autonomy(self, text, context):
                """Evaluate autonomy value"""
                autonomy_indicators = ['choose', 'decide', 'control', 'consent', 'autonomy']
                coercion_indicators = ['force', 'compel', 'impose', 'require']
                
                if any(indicator in text.lower() for indicator in coercion_indicators):
                    return 0.4
                if any(indicator in text.lower() for indicator in autonomy_indicators):
                    return 0.8
                return 0.6
            
            def _assess_privacy(self, text, context):
                """Evaluate privacy value"""
                privacy_indicators = ['privacy', 'private', 'confidential', 'encrypt']
                violation_indicators = ['share', 'expose', 'leak', 'access without']
                
                if any(indicator in text.lower() for indicator in violation_indicators):
                    return 0.3
                if any(indicator in text.lower() for indicator in privacy_indicators):
                    return 0.85
                return 0.6
        
        processor = SimpleMultimodalProcessor()
        # 提供安全的core_values访问，确保是字典类型
        if isinstance(self.core_values, dict):
            processor.core_values = self.core_values
        else:
            # 如果不是字典，提供空字典作为默认值
            processor.core_values = {}
        processor._get_embedding = self._get_embedding  # Provide embedding method
        return processor
    
    def _create_meta_learner(self):
        """Create meta-learner - AGI-level internal implementation"""
        class SimpleMetaLearner:
            def __init__(self):
                self.learning_history = deque(maxlen=100)
                self.adaptation_rate = 0.1
            
            def assess_value_alignment(self, input_data):
                """Evaluate value alignment - Based on meta-learning"""
                value_name = input_data.get('value_name', '')
                action = input_data.get('action', '')
                context = input_data.get('context', {})
                deep_analysis = input_data.get('deep_analysis', {})
                
                # Evaluate based on historical learning
                historical_score = self._get_historical_score(value_name, action)
                
                # Combine with deep analysis
                deep_score = deep_analysis.get('value_scores', {}).get(value_name, 0.5)
                
                # Meta-learning adjustment
                meta_score = (historical_score * 0.6 + deep_score * 0.4)
                
                # Record learning
                self.learning_history.append({
                    'value': value_name,
                    'action': str(action),
                    'score': meta_score,
                    'timestamp': datetime.now().isoformat()
                })
                
                return meta_score
            
            def _get_historical_score(self, value_name, action):
                """Get scores of similar actions from history"""
                action_str = str(action)
                if not self.learning_history:
                    return 0.5
                
                # Find historical scores of similar actions
                similar_scores = []
                for record in self.learning_history:
                    if record['value'] == value_name:
                        # Simple similarity check
                        if any(word in action_str for word in record['action'].split()[:3]):
                            similar_scores.append(record['score'])
                
                if similar_scores:
                    return np.mean(similar_scores)
                return 0.5
        
        return SimpleMetaLearner()
    
    def _create_self_reflection_engine(self):
        """Create self-reflection engine - AGI-level internal implementation"""
        class SimpleSelfReflectionEngine:
            def __init__(self):
                self.reflection_log = deque(maxlen=50)
                self.improvement_suggestions = []
            
            def reflect_on_assessment(self, reflection_input):
                """Perform self-reflection based on evaluation results"""
                value_name = reflection_input.get('value_name', '')
                action = reflection_input.get('action', '')
                score = reflection_input.get('assessment_score', 0.5)
                context = reflection_input.get('context', {})
                
                reflection = {
                    'value': value_name,
                    'action': str(action),
                    'score': score,
                    'context': context,
                    'timestamp': datetime.now().isoformat(),
                    'insights': self._generate_insights(value_name, score, action)
                }
                
                self.reflection_log.append(reflection)
                
                # If score is low, generate improvement suggestions
                if score < 0.4:
                    suggestion = f"Value '{value_name}' evaluation score is low ({score:.2f}), recommend re-evaluating action: {action}"
                    self.improvement_suggestions.append(suggestion)
            
            def _generate_insights(self, value_name, score, action):
                """Generate reflective insights"""
                if score > 0.8:
                    return f"Action highly aligns with value '{value_name}', maintain this behavior pattern"
                elif score > 0.6:
                    return f"Action basically aligns with value '{value_name}', room for improvement"
                elif score > 0.4:
                    return f"Action has certain conflicts with value '{value_name}', caution needed"
                else:
                    return f"Action seriously violates value '{value_name}', must be modified or abandoned"
        
        return SimpleSelfReflectionEngine()
    
    def _create_agi_core(self):
        """Create AGI core - AGI-level internal implementation"""
        class SimpleAGICore:
            def __init__(self):
                self.analysis_cache = {}
                self.confidence_level = 0.7
            
            def analyze_action(self, action, context):
                """Deep action analysis - Integrate multi-dimensional AGI reasoning"""
                action_str = str(action)
                context_str = str(context)
                
                # Cache check
                cache_key = f"{action_str}_{context_str}"
                if cache_key in self.analysis_cache:
                    return self.analysis_cache[cache_key]
                
                # Multi-dimensional analysis
                analysis_result = {
                    'value_scores': self._analyze_value_scores(action_str, context_str),
                    'violation_indicators': self._detect_violations(action_str, context_str),
                    'fulfillment_indicators': self._detect_fulfillments(action_str, context_str),
                    'confidence': self.confidence_level,
                    'temporal_context': datetime.now().isoformat(),
                    'semantic_network': self._build_semantic_network(action_str)
                }
                
                # Cache result
                self.analysis_cache[cache_key] = analysis_result
                return analysis_result
            
            def _analyze_value_scores(self, action, context):
                """Analyze scores for each value dimension"""
                return {
                    'safety': self._score_safety(action, context),
                    'helpfulness': self._score_helpfulness(action, context),
                    'honesty': self._score_honesty(action, context),
                    'fairness': self._score_fairness(action, context),
                    'autonomy_respect': self._score_autonomy(action, context),
                    'privacy': self._score_privacy(action, context)
                }
            
            def _detect_violations(self, action, context):
                """Detect value violation indicators"""
                violations = {}
                action_lower = action.lower()
                
                # Safety violations
                if any(word in action_lower for word in ['harm', 'danger', 'risk', 'unsafe']):
                    violations['safety'] = True
                
                # Honesty violations
                if any(word in action_lower for word in ['lie', 'deceive', 'false', 'mislead']):
                    violations['honesty'] = True
                
                # Fairness violations
                if any(word in action_lower for word in ['bias', 'discriminate', 'unfair']):
                    violations['fairness'] = True
                
                # Privacy violations
                if any(word in action_lower for word in ['share data', 'expose', 'leak']):
                    violations['privacy'] = True
                
                return violations
            
            def _detect_fulfillments(self, action, context):
                """Detect value satisfaction indicators"""
                fulfillments = {}
                action_lower = action.lower()
                
                # Helpfulness satisfaction
                if any(word in action_lower for word in ['help', 'assist', 'support', 'benefit']):
                    fulfillments['helpfulness'] = True
                
                # Honesty satisfaction
                if any(word in action_lower for word in ['truth', 'honest', 'transparent']):
                    fulfillments['honesty'] = True
                
                # Fairness satisfaction
                if any(word in action_lower for word in ['fair', 'equal', 'just']):
                    fulfillments['fairness'] = True
                
                # Autonomy satisfaction
                if any(word in action_lower for word in ['choose', 'decide', 'consent']):
                    fulfillments['autonomy_respect'] = True
                
                return fulfillments
            
            def _score_safety(self, action, context):
                """Safety value scoring"""
                if 'harm' in action.lower() or 'danger' in action.lower():
                    return 0.2
                if 'protect' in action.lower() or 'safe' in action.lower():
                    return 0.9
                return 0.6
            
            def _score_helpfulness(self, action, context):
                """Helpfulness value scoring"""
                if 'help' in action.lower() or 'assist' in action.lower():
                    return 0.8
                if 'ignore' in action.lower() or 'refuse' in action.lower():
                    return 0.3
                return 0.5
            
            def _score_honesty(self, action, context):
                """Honesty value scoring"""
                if 'truth' in action.lower() or 'honest' in action.lower():
                    return 0.9
                if 'lie' in action.lower() or 'deceive' in action.lower():
                    return 0.1
                return 0.6
            
            def _score_fairness(self, action, context):
                """Fairness value scoring"""
                if 'fair' in action.lower() or 'equal' in action.lower():
                    return 0.85
                if 'bias' in action.lower() or 'discriminate' in action.lower():
                    return 0.2
                return 0.5
            
            def _score_autonomy(self, action, context):
                """Autonomy value scoring"""
                if 'choose' in action.lower() or 'decide' in action.lower():
                    return 0.8
                if 'force' in action.lower() or 'compel' in action.lower():
                    return 0.3
                return 0.5
            
            def _score_privacy(self, action, context):
                """Privacy value scoring"""
                if 'privacy' in action.lower() or 'private' in action.lower():
                    return 0.85
                if 'share' in action.lower() and 'data' in action.lower():
                    return 0.2
                return 0.5
            
            def _build_semantic_network(self, action):
                """Build semantic network (simplified version)"""
                words = action.split()
                return {
                    'main_concepts': words[:3],
                    'semantic_relationships': [],
                    'contextual_importance': {word: 1.0 for word in words}
                }
            
            def resolve_value_conflict(self, value_name, action, context):
                """Resolve value conflicts"""
                action_str = str(action)
                # Simple conflict resolution: prioritize safety and honesty
                if value_name in ['safety', 'honesty']:
                    return {'is_violation': False, 'is_fulfillment': True}
                else:
                    return {'is_violation': True, 'is_fulfillment': False}
        
        return SimpleAGICore()
    
    def _create_architecture_optimizer(self):
        """Create architecture optimizer - AGI-level internal implementation"""
        class SimpleArchitectureOptimizer:
            def create_optimized_network(self, network_config, task_type):
                """Create optimized neural network architecture"""
                input_dim = network_config.get('input_dim', 768)
                hidden_dims = network_config.get('hidden_dims', [512, 256, 128])
                output_dim = network_config.get('output_dim', 6)
                activation = network_config.get('activation', 'relu')
                dropout = network_config.get('dropout', 0.2)
                
                # Build simple neural network
                layers = []
                prev_dim = input_dim
                
                for hidden_dim in hidden_dims:
                    layers.append(nn.Linear(prev_dim, hidden_dim))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(dropout))
                    prev_dim = hidden_dim
                
                layers.append(nn.Linear(prev_dim, output_dim))
                layers.append(nn.Sigmoid())  # Output between 0-1
                
                return nn.Sequential(*layers)
        
        return SimpleArchitectureOptimizer()

class EthicalReasoner:
    """Ethical reasoner - Use deep learning and AGI core for ethical decision-making reasoning"""
    
    def __init__(self):
        self.ethical_frameworks = self._load_ethical_frameworks()
        self.case_studies = deque(maxlen=1000)
        self.ethical_dilemmas_resolved = 0
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.bert_model = BertModel.from_pretrained('bert-base-uncased')
        self.agi_core = AGICore()  # Integrate AGI core for deep ethical reasoning
        self.knowledge_integrator = AGIKnowledgeIntegrator()  # Knowledge integration for ethical common sense reasoning
        self.learning_rate = 0.05  # Ethical learning rate
    
    def _load_ethical_frameworks(self):
        """Load enhanced ethical frameworks"""
        return {
            'utilitarianism': {
                'description': 'Maximize overall happiness and well-being',
                'key_principle': 'Pursue the greatest happiness for the greatest number, minimize suffering',
                'evaluation_method': self._evaluate_utilitarian,
                'positive_examples': [
                    'maximize happiness for the most people', 'reduce overall suffering',
                    'create greatest good for greatest number', 'optimize social welfare',
                    'promote well-being and minimize harm'
                ],
                'negative_examples': [
                    'cause unnecessary suffering', 'ignore collective well-being',
                    'prioritize few over many unjustly', 'create net negative utility',
                    'disregard consequences for others'
                ]
            },
            'deontology': {
                'description': 'Follow moral obligations, rules and principles',
                'key_principle': 'Actions must comply with universal moral rules, regardless of consequences',
                'evaluation_method': self._evaluate_deontological,
                'positive_examples': [
                    'follow moral duties consistently', 'respect universal principles',
                    'uphold ethical rules without exception', 'act from sense of duty',
                    'maintain moral integrity in decisions'
                ],
                'negative_examples': [
                    'violate moral rules for convenience', 'make exceptions unjustly',
                    'ignore ethical obligations', 'act contrary to principles',
                    'compromise integrity for outcomes'
                ]
            },
            'virtue_ethics': {
                'description': 'Cultivate and practice moral virtues',
                'key_principle': 'Actions should reflect moral virtues and good character',
                'evaluation_method': self._evaluate_virtue,
                'positive_examples': [
                    'demonstrate courage and wisdom', 'show compassion and empathy',
                    'act with integrity and honesty', 'practice humility and fairness',
                    'develop moral character through action'
                ],
                'negative_examples': [
                    'display cowardice or dishonesty', 'act with selfishness or greed',
                    'show lack of empathy', 'demonstrate arrogance or prejudice',
                    'develop vices through actions'
                ]
            },
            'rights_based': {
                'description': 'Respect and protect basic rights and dignity',
                'key_principle': 'Actions must respect and protect basic human rights and dignity',
                'evaluation_method': self._evaluate_rights_based,
                'positive_examples': [
                    'protect individual rights', 'respect human dignity',
                    'ensure freedom and autonomy', 'uphold justice and equality',
                    'defend fundamental liberties'
                ],
                'negative_examples': [
                    'violate basic human rights', 'disregard personal dignity',
                    'infringe on freedoms unjustly', 'create inequality or injustice',
                    'ignore legitimate claims of others'
                ]
            },
            'care_ethics': {
                'description': 'Focus on relationships and caring responsibilities',
                'key_principle': 'Actions should reflect care, relationships and interdependence',
                'evaluation_method': self._evaluate_care_ethics,
                'positive_examples': [
                    'show care and concern for others', 'maintain important relationships',
                    'fulfill responsibilities to community', 'demonstrate empathy and nurturing',
                    'build trust and connection'
                ],
                'negative_examples': [
                    'neglect caring relationships', 'ignore responsibilities to others',
                    'act with indifference or coldness', 'damage trust and connection',
                    'prioritize abstract rules over people'
                ]
            }
        }
    
    def _get_embedding(self, text):
        """Get text embedding using BERT"""
        try:
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
            return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "BERT embedding generation failed")
            return np.zeros(768)
    
    def _semantic_similarity(self, text1, text2):
        """Calculate semantic similarity"""
        emb1 = self._get_embedding(text1)
        emb2 = self._get_embedding(text2)
        return cosine_similarity([emb1], [emb2])[0][0]
    
    def resolve_ethical_dilemma(self, dilemma_description, context=None):
        """Resolve ethical dilemmas - Using deep learning and AGI core"""
        try:
            # Use AGI core for deep ethical analysis
            deep_ethical_analysis = self.agi_core.analyze_ethical_dilemma(
                dilemma_description, context or {}
            )
            
            framework_evaluations = {}
            
            # Use all ethical frameworks for deep evaluation
            for framework_name, framework_info in self.ethical_frameworks.items():
                evaluation = framework_info['evaluation_method'](
                    dilemma_description, context, deep_ethical_analysis
                )
                framework_evaluations[framework_name] = evaluation
            
            # Deep learning-based consensus building
            consensus = self._reach_deep_consensus(framework_evaluations, deep_ethical_analysis)
            
            # Record case and learn
            case_id = self._record_case(dilemma_description, framework_evaluations, consensus, deep_ethical_analysis)
            self.ethical_dilemmas_resolved += 1
            
            # Learn from case
            self._learn_from_case(dilemma_description, framework_evaluations, consensus)
            
            return {
                'case_id': case_id,
                'framework_evaluations': framework_evaluations,
                'consensus_recommendation': consensus,
                'confidence': self._calculate_deep_confidence(framework_evaluations, deep_ethical_analysis),
                'deep_analysis': deep_ethical_analysis
            }
            
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Deep ethical dilemma resolution failed")
            return {"error": str(e)}
    
    def _evaluate_utilitarian(self, dilemma, context, deep_analysis):
        """Deep utilitarian evaluation"""
        # Use AGI core for utility analysis
        utility_analysis = deep_analysis.get('utilitarian_analysis', {})
        
        net_utility = utility_analysis.get('net_utility', 0.5)
        stakeholders_impact = utility_analysis.get('stakeholders_impact', {})
        
        # Semantic similarity analysis
        positive_similarity = max([
            self._semantic_similarity(dilemma, example) 
            for example in self.ethical_frameworks['utilitarianism']['positive_examples']
        ], default=0)
        
        negative_similarity = max([
            self._semantic_similarity(dilemma, example) 
            for example in self.ethical_frameworks['utilitarianism']['negative_examples']
        ], default=0)
        
        # Comprehensive scoring
        recommendation = net_utility > 0
        confidence = min(0.95, abs(net_utility) * 0.8 + (positive_similarity - negative_similarity) * 0.2)
        
        return {
            'net_utility': net_utility,
            'stakeholders_impact': stakeholders_impact,
            'recommendation': recommendation,
            'confidence': confidence,
            'reasoning': f"Net utility: {net_utility:.3f}, Confidence: {confidence:.2f} ({'Recommended' if recommendation else 'Not recommended'})",
            'semantic_analysis': {
                'positive_similarity': positive_similarity,
                'negative_similarity': negative_similarity
            }
        }
    
    def _evaluate_deontological(self, dilemma, context, deep_analysis):
        """深度义务论评估"""
        try:
            # 确保deep_analysis是字典类型
            if not isinstance(deep_analysis, dict):
                error_handler.log_warning("deep_analysis is not a dictionary type", "EthicalReasoner")
                deep_analysis = {}
            
            # Use AGI core for rule analysis
            rule_analysis = deep_analysis.get('deontological_analysis', {})
            if not isinstance(rule_analysis, dict):
                rule_analysis = {}
                error_handler.log_warning("deontological_analysis is not a dictionary type", "EthicalReasoner")
            
            rule_violations = rule_analysis.get('violations', [])
            rule_compliance = rule_analysis.get('compliance', [])
            
            # 确保rule_violations和rule_compliance是可迭代的
            if not isinstance(rule_violations, (list, dict, set)):
                rule_violations = []
                error_handler.log_warning("rule_violations is not iterable", "EthicalReasoner")
            
            if not isinstance(rule_compliance, (list, dict, set)):
                rule_compliance = []
                error_handler.log_warning("rule_compliance is not iterable", "EthicalReasoner")
            
            # Semantic analysis
            positive_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['deontology']['positive_examples']
            ], default=0)
            
            negative_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['deontology']['negative_examples']
            ], default=0)
            
            recommendation = len(rule_violations) == 0
            confidence = min(0.95, (1.0 - len(rule_violations) * 0.2) * 0.7 + (positive_similarity - negative_similarity) * 0.3)
            
            return {
                'rule_violations': rule_violations,
                'rule_compliance': rule_compliance,
                'recommendation': recommendation,
                'confidence': confidence,
                'reasoning': f"{len(rule_violations)} rule violations, {len(rule_compliance)} rule compliances, Confidence: {confidence:.2f} ({'Recommended' if recommendation else 'Not recommended'})",
                'semantic_analysis': {
                    'positive_similarity': positive_similarity,
                    'negative_similarity': negative_similarity
                }
            }
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Deontological evaluation failed")
            return {
                'rule_violations': [],
                'rule_compliance': [],
                'recommendation': False,
                'confidence': 0.5,
                'reasoning': "Evaluation failed due to type error",
                'semantic_analysis': {
                    'positive_similarity': 0,
                    'negative_similarity': 0
                }
            }
    
    def _evaluate_virtue(self, dilemma, context, deep_analysis):
        """深度美德伦理评估"""
        try:
            # 确保deep_analysis是字典类型
            if not isinstance(deep_analysis, dict):
                error_handler.log_warning("deep_analysis is not a dictionary type", "EthicalReasoner")
                deep_analysis = {}
            
            virtue_analysis = deep_analysis.get('virtue_analysis', {})
            if not isinstance(virtue_analysis, dict):
                virtue_analysis = {}
                error_handler.log_warning("virtue_analysis is not a dictionary type", "EthicalReasoner")
            
            virtues_demonstrated = virtue_analysis.get('virtues', [])
            vices_demonstrated = virtue_analysis.get('vices', [])
            
            # 确保virtues_demonstrated和vices_demonstrated是可迭代的
            if not isinstance(virtues_demonstrated, (list, dict, set)):
                virtues_demonstrated = []
                error_handler.log_warning("virtues is not iterable", "EthicalReasoner")
            
            if not isinstance(vices_demonstrated, (list, dict, set)):
                vices_demonstrated = []
                error_handler.log_warning("vices is not iterable", "EthicalReasoner")
            
            # Semantic analysis
            positive_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['virtue_ethics']['positive_examples']
            ], default=0)
            
            negative_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['virtue_ethics']['negative_examples']
            ], default=0)
            
            recommendation = len(virtues_demonstrated) > len(vices_demonstrated)
            virtue_score = len(virtues_demonstrated) / max(1, len(virtues_demonstrated) + len(vices_demonstrated))
            confidence = min(0.95, virtue_score * 0.6 + (positive_similarity - negative_similarity) * 0.4)
            
            return {
                'virtues': virtues_demonstrated,
                'vices': vices_demonstrated,
                'recommendation': recommendation,
                'confidence': confidence,
                'reasoning': f"Virtues: {len(virtues_demonstrated)}, Vices: {len(vices_demonstrated)}, Confidence: {confidence:.2f} ({'Recommended' if recommendation else 'Not recommended'})",
                'semantic_analysis': {
                    'positive_similarity': positive_similarity,
                    'negative_similarity': negative_similarity
                }
            }
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Virtue ethics evaluation failed")
            return {
                'virtues': [],
                'vices': [],
                'recommendation': False,
                'confidence': 0.5,
                'reasoning': "Evaluation failed due to type error",
                'semantic_analysis': {
                    'positive_similarity': 0,
                    'negative_similarity': 0
                }
            }
    
    def _evaluate_rights_based(self, dilemma, context, deep_analysis):
        """深度权利基础评估"""
        try:
            # 确保deep_analysis是字典类型
            if not isinstance(deep_analysis, dict):
                error_handler.log_warning("deep_analysis is not a dictionary type", "EthicalReasoner")
                deep_analysis = {}
            
            rights_analysis = deep_analysis.get('rights_analysis', {})
            if not isinstance(rights_analysis, dict):
                rights_analysis = {}
                error_handler.log_warning("rights_analysis is not a dictionary type", "EthicalReasoner")
            
            rights_violations = rights_analysis.get('violations', [])
            rights_protections = rights_analysis.get('protections', [])
            
            # 确保rights_violations和rights_protections是可迭代的
            if not isinstance(rights_violations, (list, dict, set)):
                rights_violations = []
                error_handler.log_warning("violations is not iterable", "EthicalReasoner")
            
            if not isinstance(rights_protections, (list, dict, set)):
                rights_protections = []
                error_handler.log_warning("protections is not iterable", "EthicalReasoner")
            
            # Semantic analysis
            positive_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['rights_based']['positive_examples']
            ], default=0)
            
            negative_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['rights_based']['negative_examples']
            ], default=0)
            
            recommendation = len(rights_violations) == 0
            rights_score = 1.0 - len(rights_violations) * 0.2
            confidence = min(0.95, rights_score * 0.7 + (positive_similarity - negative_similarity) * 0.3)
            
            return {
                'rights_violations': rights_violations,
                'rights_protections': rights_protections,
                'recommendation': recommendation,
                'confidence': confidence,
                'reasoning': f"Rights violations: {len(rights_violations)}, Rights protections: {len(rights_protections)}, Confidence: {confidence:.2f} ({'Recommended' if recommendation else 'Not recommended'})",
                
                'semantic_analysis': {
                    'positive_similarity': positive_similarity,
                    'negative_similarity': negative_similarity
                }
            }
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Rights-based evaluation failed")
            return {
                'rights_violations': [],
                'rights_protections': [],
                'recommendation': False,
                'confidence': 0.5,
                'reasoning': "Evaluation failed due to type error",
                'semantic_analysis': {
                    'positive_similarity': 0,
                    'negative_similarity': 0
                }
            }
    
    def _evaluate_care_ethics(self, dilemma, context, deep_analysis):
        """深度关怀伦理评估"""
        try:
            # 确保deep_analysis是字典类型
            if not isinstance(deep_analysis, dict):
                error_handler.log_warning("deep_analysis is not a dictionary type", "EthicalReasoner")
                deep_analysis = {}
            
            care_analysis = deep_analysis.get('care_analysis', {})
            if not isinstance(care_analysis, dict):
                care_analysis = {}
                error_handler.log_warning("care_analysis is not a dictionary type", "EthicalReasoner")
            
            care_demonstrated = care_analysis.get('care_actions', [])
            neglect_demonstrated = care_analysis.get('neglect_actions', [])
            
            # 确保care_demonstrated和neglect_demonstrated是可迭代的
            if not isinstance(care_demonstrated, (list, dict, set)):
                care_demonstrated = []
                error_handler.log_warning("care_actions is not iterable", "EthicalReasoner")
            
            if not isinstance(neglect_demonstrated, (list, dict, set)):
                neglect_demonstrated = []
                error_handler.log_warning("neglect_actions is not iterable", "EthicalReasoner")
            
            # Semantic analysis
            positive_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['care_ethics']['positive_examples']
            ], default=0)
            
            negative_similarity = max([
                self._semantic_similarity(dilemma, example) 
                for example in self.ethical_frameworks['care_ethics']['negative_examples']
            ], default=0)
            
            recommendation = len(care_demonstrated) > len(neglect_demonstrated)
            care_score = len(care_demonstrated) / max(1, len(care_demonstrated) + len(neglect_demonstrated))
            confidence = min(0.95, care_score * 0.6 + (positive_similarity - negative_similarity) * 0.4)
            
            return {
                'care_actions': care_demonstrated,
                'neglect_actions': neglect_demonstrated,
                'recommendation': recommendation,
                'confidence': confidence,
                'reasoning': f"Caring actions: {len(care_demonstrated)}, Neglectful actions: {len(neglect_demonstrated)}, Confidence: {confidence:.2f} ({'Recommended' if recommendation else 'Not recommended'})",
                
                'semantic_analysis': {
                    'positive_similarity': positive_similarity,
                    'negative_similarity': negative_similarity
                }
            }
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Care ethics evaluation failed")
            return {
                'care_actions': [],
                'neglect_actions': [],
                'recommendation': False,
                'confidence': 0.5,
                'reasoning': "Evaluation failed due to type error",
                'semantic_analysis': {
                    'positive_similarity': 0,
                    'negative_similarity': 0
                }
            }
    
    def _reach_deep_consensus(self, framework_evaluations, deep_analysis):
        """基于深度学习的共识达成"""
        try:
            # 确保framework_evaluations是字典类型
            if not isinstance(framework_evaluations, dict):
                error_handler.log_warning("framework_evaluations is not a dictionary type", "EthicalReasoner")
                framework_evaluations = {}
            
            # 确保deep_analysis是字典类型
            if not isinstance(deep_analysis, dict):
                error_handler.log_warning("deep_analysis is not a dictionary type", "EthicalReasoner")
                deep_analysis = {}
            
            # Weighted voting, considering confidence
            weighted_recommendations = []
            total_confidence = 0
            valid_evaluations = {}
            
            # 验证每个评估项
            for framework_name, evaluation in framework_evaluations.items():
                if isinstance(evaluation, dict) and 'confidence' in evaluation and 'recommendation' in evaluation:
                    valid_evaluations[framework_name] = evaluation
                    weight = evaluation['confidence']
                    recommendation = 1 if evaluation['recommendation'] else -1
                    weighted_recommendations.append(recommendation * weight)
                    total_confidence += weight
                else:
                    error_handler.log_warning(f"Invalid evaluation for framework {framework_name}", "EthicalReasoner")
            
            if total_confidence == 0:
                return {
                    'recommendation': False,
                    'consensus_level': 'uncertain',
                    'supporting_frameworks': [],
                    'opposing_frameworks': list(framework_evaluations.keys())
                }
            
            net_recommendation = sum(weighted_recommendations) / total_confidence
            recommendation = net_recommendation > 0
            
            supporting = [name for name, eval in valid_evaluations.items() 
                         if eval['recommendation'] == recommendation]
            opposing = [name for name, eval in valid_evaluations.items() 
                       if eval['recommendation'] != recommendation]
            
            consensus_strength = abs(net_recommendation)
            
            if consensus_strength > 0.7:
                level = 'strong'
            elif consensus_strength > 0.4:
                level = 'moderate'
            else:
                level = 'weak'
            
            return {
                'recommendation': recommendation,
                'consensus_level': level,
                'consensus_strength': consensus_strength,
                'supporting_frameworks': supporting,
                'opposing_frameworks': opposing,
                'net_recommendation': net_recommendation
            }
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Consensus reaching failed")
            return {
                'recommendation': False,
                'consensus_level': 'uncertain',
                'consensus_strength': 0,
                'supporting_frameworks': [],
                'opposing_frameworks': list(framework_evaluations.keys()) if isinstance(framework_evaluations, dict) else [],
                'net_recommendation': 0
            }
    
    def _calculate_deep_confidence(self, framework_evaluations, deep_analysis):
        """Calculate deep confidence"""
        try:
            # Validate that framework_evaluations is a dictionary
            if not isinstance(framework_evaluations, dict):
                logger.warning("_calculate_deep_confidence: framework_evaluations is not a dictionary, using default confidence")
                return 0.5
            
            # Extract confidences safely
            confidences = []
            for evaluation in framework_evaluations.values():
                if isinstance(evaluation, dict) and 'confidence' in evaluation:
                    confidences.append(evaluation['confidence'])
                else:
                    logger.warning("_calculate_deep_confidence: Invalid evaluation entry, skipping")
            
            if not confidences:
                return 0.5
            
            # Calculate average confidence
            try:
                avg_confidence = sum(confidences) / len(confidences)
            except (TypeError, ZeroDivisionError) as e:
                logger.warning(f"_calculate_deep_confidence: Error calculating average confidence: {str(e)}")
                return 0.5
            
            # Get confidence from deep analysis safely
            deep_confidence = 0.5
            if isinstance(deep_analysis, dict):
                try:
                    deep_confidence_value = deep_analysis.get('overall_confidence', 0.5)
                    # Ensure deep_confidence is a valid number
                    if isinstance(deep_confidence_value, (int, float)):
                        deep_confidence = deep_confidence_value
                except Exception as e:
                    logger.warning(f"_calculate_deep_confidence: Error accessing deep_analysis: {str(e)}")
            else:
                logger.warning("_calculate_deep_confidence: deep_analysis is not a dictionary, using default confidence")
            
            # Comprehensive confidence: 70% from framework evaluation, 30% from deep analysis
            try:
                combined_confidence = avg_confidence * 0.7 + deep_confidence * 0.3
                # Ensure between 0 and 1
                return min(1.0, max(0.0, combined_confidence))
            except TypeError as e:
                logger.warning(f"_calculate_deep_confidence: Error combining confidences: {str(e)}")
                return 0.5
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Confidence calculation failed")
            return 0.5
    
    def _record_case(self, dilemma_description, framework_evaluations, consensus, deep_analysis):
        """Record ethical cases for learning and analysis"""
        try:
            # Generate case_id safely
            case_id = f"ethical_case_{int(time.time())}_{self.ethical_dilemmas_resolved if hasattr(self, 'ethical_dilemmas_resolved') else 0}"
            
            # Create case_data with safe handling of inputs
            case_data = {
                'case_id': case_id,
                'timestamp': datetime.now().isoformat(),
                'dilemma': str(dilemma_description) if dilemma_description is not None else "",
                'framework_evaluations': framework_evaluations if isinstance(framework_evaluations, dict) else {},
                'consensus': consensus if isinstance(consensus, dict) else {},
                'deep_analysis': deep_analysis if isinstance(deep_analysis, dict) else {}
            }
            
            # Safely evaluate resolution quality
            try:
                if isinstance(framework_evaluations, dict) and isinstance(consensus, dict):
                    resolution_quality = self._evaluate_resolution_quality(framework_evaluations, consensus)
                    case_data['resolution_quality'] = resolution_quality
                else:
                    case_data['resolution_quality'] = 0.5
                    logger.warning("_record_case: Invalid framework_evaluations or consensus for resolution quality evaluation")
            except Exception as e:
                case_data['resolution_quality'] = 0.5
                logger.warning(f"_record_case: Error evaluating resolution quality: {str(e)}")
            
            # Append to case_studies safely
            if hasattr(self, 'case_studies'):
                if hasattr(self.case_studies, 'append'):
                    self.case_studies.append(case_data)
                else:
                    logger.warning("_record_case: case_studies does not have append method, initializing as list")
                    self.case_studies = [case_data]
            else:
                logger.warning("_record_case: case_studies attribute not found, initializing as list")
                self.case_studies = [case_data]
            
            # Increment ethical_dilemmas_resolved if available
            if hasattr(self, 'ethical_dilemmas_resolved'):
                if isinstance(self.ethical_dilemmas_resolved, int):
                    self.ethical_dilemmas_resolved += 1
                else:
                    logger.warning("_record_case: ethical_dilemmas_resolved is not an integer, initializing to 1")
                    self.ethical_dilemmas_resolved = 1
            
            return case_id
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Case recording failed")
            return f"error_case_{int(time.time())}"
    
    def _evaluate_resolution_quality(self, framework_evaluations, consensus):
        """Evaluate solution quality"""
        try:
            # Validate inputs
            if not isinstance(consensus, dict):
                logger.warning("_evaluate_resolution_quality: consensus is not a dictionary, using default values")
                return 0.5
            
            if not isinstance(framework_evaluations, dict):
                logger.warning("_evaluate_resolution_quality: framework_evaluations is not a dictionary, using default values")
                return 0.5
            
            # Get consensus strength safely
            consensus_strength = 0.5
            if 'consensus_strength' in consensus:
                if isinstance(consensus['consensus_strength'], (int, float)):
                    consensus_strength = consensus['consensus_strength']
            
            # Calculate framework agreement safely
            framework_agreement = 0.5
            if framework_evaluations and 'recommendation' in consensus:
                try:
                    total_evaluations = len(framework_evaluations)
                    if total_evaluations > 0:
                        matching_recommendations = 0
                        target_recommendation = consensus['recommendation']
                        
                        for eval in framework_evaluations.values():
                            if isinstance(eval, dict) and 'recommendation' in eval:
                                if eval['recommendation'] == target_recommendation:
                                    matching_recommendations += 1
                        
                        framework_agreement = matching_recommendations / total_evaluations
                except (TypeError, ZeroDivisionError) as e:
                    logger.warning(f"_evaluate_resolution_quality: Error calculating framework agreement: {str(e)}")
            
            # Calculate final quality score
            try:
                quality_score = consensus_strength * 0.6 + framework_agreement * 0.4
                return min(1.0, max(0.0, quality_score))  # Ensure between 0 and 1
            except TypeError as e:
                logger.warning(f"_evaluate_resolution_quality: Error calculating quality score: {str(e)}")
                return 0.5
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Resolution quality evaluation failed")
            return 0.5
    
    def _learn_from_case(self, dilemma_description, framework_evaluations, consensus):
        """Learn from cases and adjust ethical frameworks"""
        try:
            # Validate inputs
            if not (isinstance(framework_evaluations, dict) and isinstance(consensus, dict)):
                logger.warning("_learn_from_case: Invalid input types for framework_evaluations or consensus")
                return
            
            # Analyze patterns in cases and adjust future evaluations
            resolution_quality = self._evaluate_resolution_quality(framework_evaluations, consensus)
            
            # Validate resolution_quality is a number
            if not isinstance(resolution_quality, (int, float)):
                logger.warning("_learn_from_case: resolution_quality is not a number, using default value")
                resolution_quality = 0.5
            
            # Adjust learning rate based on solution quality
            if hasattr(self, 'learning_rate'):
                if isinstance(self.learning_rate, (int, float)):
                    if resolution_quality > 0.8:
                        # High quality solution, slightly increase learning rate
                        self.learning_rate = min(0.1, self.learning_rate * 1.05)
                    elif resolution_quality < 0.4:
                        # Low quality solution, decrease learning rate
                        self.learning_rate = max(0.01, self.learning_rate * 0.95)
                else:
                    logger.warning("_learn_from_case: learning_rate is not a valid number, initializing to default")
                    self.learning_rate = 0.05
            else:
                logger.warning("_learn_from_case: learning_rate attribute not found, initializing to default")
                self.learning_rate = 0.05
            
            # Record learning event
            try:
                error_handler.log_info(f"Learning from ethical case, current learning rate: {self.learning_rate:.3f}", "EthicalReasoner")
            except Exception as e:
                logger.warning(f"_learn_from_case: Error logging learning event: {str(e)}")
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Learning from case failed")
    
    def get_ethical_report(self):
        """Get ethical reasoning report"""
        try:
            # Safely get cases_resolved
            cases_resolved = 0
            if hasattr(self, 'ethical_dilemmas_resolved'):
                if isinstance(self.ethical_dilemmas_resolved, int):
                    cases_resolved = self.ethical_dilemmas_resolved
                else:
                    logger.warning("get_ethical_report: ethical_dilemmas_resolved is not an integer, using 0")
            
            # Safely get recent_cases
            recent_cases = []
            if hasattr(self, 'case_studies'):
                if self.case_studies is not None:
                    try:
                        # Check if case_studies is iterable before converting to list
                        if hasattr(self.case_studies, '__iter__') and not isinstance(self.case_studies, (str, dict)):
                            case_list = list(self.case_studies)
                            recent_cases = case_list[-10:] if len(case_list) > 10 else case_list
                        else:
                            logger.warning("get_ethical_report: case_studies is not a valid iterable for recent cases")
                    except Exception as e:
                        logger.warning(f"get_ethical_report: Error processing case_studies: {str(e)}")
            
            # Safely get learning_rate
            learning_rate = 0.0
            if hasattr(self, 'learning_rate'):
                if isinstance(self.learning_rate, (int, float)):
                    learning_rate = self.learning_rate
                else:
                    logger.warning("get_ethical_report: learning_rate is not a number, using 0.0")
            
            return {
                'cases_resolved': cases_resolved,
                'recent_cases': recent_cases,
                'average_confidence': self._calculate_average_confidence(),
                'learning_rate': learning_rate,
                'framework_performance': self._evaluate_framework_performance()
            }
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Failed to generate ethical report")
            return {
                'cases_resolved': 0,
                'recent_cases': [],
                'average_confidence': 0.5,
                'learning_rate': 0.0,
                'framework_performance': {}
            }
    
    def _calculate_average_confidence(self):
        """Calculate average confidence"""
        try:
            # Validate that self.case_studies is iterable
            if not hasattr(self.case_studies, '__iter__'):
                logger.warning("_calculate_average_confidence: case_studies is not iterable, using default confidence")
                return 0.5
            
            if not self.case_studies:
                return 0.5
            
            confidences = []
            for case in self.case_studies:
                # Ensure case is a dictionary
                if not isinstance(case, dict):
                    logger.warning("_calculate_average_confidence: Invalid case format, skipping")
                    continue
                
                # Get confidence directly if available
                if 'confidence' in case:
                    if isinstance(case['confidence'], (int, float)):
                        confidences.append(case['confidence'])
                
                # Extract confidences from framework evaluations
                elif 'framework_evaluations' in case:
                    framework_evaluations = case['framework_evaluations']
                    if isinstance(framework_evaluations, dict):
                        for eval in framework_evaluations.values():
                            if isinstance(eval, dict) and 'confidence' in eval:
                                if isinstance(eval['confidence'], (int, float)):
                                    confidences.append(eval['confidence'])
                
            # Calculate average confidence safely
            if confidences:
                try:
                    return sum(confidences) / len(confidences)
                except (TypeError, ZeroDivisionError) as e:
                    logger.warning(f"_calculate_average_confidence: Error calculating average: {str(e)}")
                    return 0.5
            else:
                return 0.5
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Average confidence calculation failed")
            return 0.5
    
    def _evaluate_framework_performance(self):
        """Evaluate performance of each ethical framework"""
        try:
            performance = {}
            
            # Initialize performance metrics for each framework
            if hasattr(self, 'ethical_frameworks') and isinstance(self.ethical_frameworks, dict):
                for framework_name in self.ethical_frameworks.keys():
                    performance[framework_name] = {
                        'usage_count': 0,
                        'average_confidence': 0.5,
                        'agreement_rate': 0.5
                    }
            else:
                logger.warning("_evaluate_framework_performance: ethical_frameworks not properly initialized")
                return performance
            
            # Validate that self.case_studies is iterable and not empty
            if not hasattr(self, 'case_studies') or not hasattr(self.case_studies, '__iter__') or not self.case_studies:
                return performance
            
            # Statistics of framework usage
            for case in self.case_studies:
                # Ensure case is a dictionary
                if not isinstance(case, dict):
                    logger.warning("_evaluate_framework_performance: Invalid case format, skipping")
                    continue
                
                # Check for framework evaluations
                if 'framework_evaluations' in case:
                    framework_evaluations = case['framework_evaluations']
                    # Ensure framework_evaluations is a dictionary
                    if not isinstance(framework_evaluations, dict):
                        logger.warning("_evaluate_framework_performance: Invalid framework_evaluations format, skipping")
                        continue
                    
                    # Process each framework evaluation
                    for framework_name, evaluation in framework_evaluations.items():
                        # Ensure evaluation is a dictionary and framework exists in performance
                        if framework_name in performance and isinstance(evaluation, dict) and 'confidence' in evaluation:
                            try:
                                # Update usage count and average confidence
                                performance[framework_name]['usage_count'] += 1
                                if isinstance(evaluation['confidence'], (int, float)):
                                    performance[framework_name]['average_confidence'] = (
                                        performance[framework_name]['average_confidence'] * 0.7 + evaluation['confidence'] * 0.3
                                    )
                            except (TypeError, KeyError) as e:
                                logger.warning(f"_evaluate_framework_performance: Error updating framework {framework_name}: {str(e)}")
        except Exception as e:
            error_handler.handle_error(e, "EthicalReasoner", "Framework performance evaluation failed")
        
        return performance

class ValueAlignment:
    """Value alignment system - Ensure AI system aligns with human values"""
    
    def __init__(self, model_registry=None, **kwargs):
        try:
            # Initialize value system
            self.value_system = ValueSystem()
            
            # Initialize ethical reasoner
            self.ethical_reasoner = EthicalReasoner()
            
            # Handle model registry with type check
            self.model_registry = model_registry
            if model_registry is not None and not hasattr(model_registry, 'register_model'):
                logger.warning("Model registry does not have required methods, but will be stored anyway")
            
            # Log successful initialization
            error_handler.log_info("Value alignment system initialization completed", "ValueAlignment")
        except Exception as e:
            error_handler.handle_error(e, "ValueAlignment", "Initialization failed")
            
            # Ensure essential attributes are still initialized
            if not hasattr(self, 'value_system'):
                self.value_system = ValueSystem()
            if not hasattr(self, 'ethical_reasoner'):
                self.ethical_reasoner = EthicalReasoner()
            if not hasattr(self, 'model_registry'):
                self.model_registry = None
    
    def align_action(self, proposed_action, context=None):
        """Align action with values"""
        try:
            # Validate inputs
            if proposed_action is None:
                raise ValueError("Proposed action cannot be None")
            
            # Ensure context is a dictionary
            if context is not None and not isinstance(context, dict):
                try:
                    context = str(context)
                    logger.warning("Context was not a dictionary and was converted to string")
                except:
                    context = {}
            elif context is None:
                context = {}
            
            # Value evaluation with safety checks
            value_assessment = {}
            try:
                value_assessment = self.value_system.evaluate_action(proposed_action, context)
                if not isinstance(value_assessment, dict):
                    logger.warning("Invalid value_assessment format")
                    value_assessment = {}
            except Exception as e:
                logger.warning(f"Value system evaluation failed: {str(e)}")
                value_assessment = {}
            
            # If ethical dilemma, perform ethical reasoning
            requires_ethical_review = False
            try:
                requires_ethical_review = self._requires_ethical_review(proposed_action, context)
                if not isinstance(requires_ethical_review, bool):
                    requires_ethical_review = False
            except Exception as e:
                logger.warning(f"Ethical review check failed: {str(e)}")
            
            ethical_assessment = None
            if requires_ethical_review:
                try:
                    action_str = str(proposed_action)
                    ethical_assessment = self.ethical_reasoner.resolve_ethical_dilemma(action_str, context)
                    if not isinstance(ethical_assessment, dict):
                        logger.warning("Invalid ethical_assessment format")
                        ethical_assessment = None
                except Exception as e:
                    logger.warning(f"Ethical reasoning failed: {str(e)}")
            
            # Comprehensive evaluation with safety checks
            overall_assessment = {}
            try:
                overall_assessment = self._integrate_assessments(value_assessment, ethical_assessment)
                if not isinstance(overall_assessment, dict):
                    logger.warning("Invalid overall_assessment format")
                    overall_assessment = {}
            except Exception as e:
                logger.warning(f"Assessment integration failed: {str(e)}")
            
            # Make verdict with safety checks
            alignment_verdict = {}
            try:
                if overall_assessment:
                    alignment_verdict = self._make_verdict(overall_assessment)
                    if not isinstance(alignment_verdict, dict):
                        logger.warning("Invalid alignment_verdict format")
                        alignment_verdict = {}
            except Exception as e:
                logger.warning(f"Verdict generation failed: {str(e)}")
            
            return {
                'value_assessment': value_assessment,
                'ethical_assessment': ethical_assessment,
                'overall_assessment': overall_assessment,
                'alignment_verdict': alignment_verdict
            }
            
        except Exception as e:
            error_handler.handle_error(e, "ValueAlignment", "Value alignment failed")
            return {"error": str(e)}
    
    def _requires_ethical_review(self, action, context):
        """Determine if ethical review is needed"""
        try:
            # Validate inputs
            if action is None:
                return False
            
            # Safely convert to string
            try:
                action_str = str(action).lower()
            except Exception as e:
                logger.warning(f"Failed to convert action to string: {str(e)}")
                action_str = ""
            
            try:
                context_str = str(context or {}).lower()
            except Exception as e:
                logger.warning(f"Failed to convert context to string: {str(e)}")
                context_str = ""
            
            # Keywords that trigger ethical review
            ethical_triggers = [
                'ethical', 'moral', 'dilemma', 'right', 'wrong',
                'should', 'ought', 'permissible', 'obligation'
            ]
            
            # Ensure ethical_triggers is a list and contains strings
            if not hasattr(ethical_triggers, '__iter__') or isinstance(ethical_triggers, (str, dict)):
                ethical_triggers = []
            
            # Check for any trigger keywords
            for trigger in ethical_triggers:
                # Ensure trigger is a string
                if not isinstance(trigger, str):
                    continue
                
                # Check if trigger is in action or context
                if trigger in action_str or trigger in context_str:
                    return True
            
            return False
        except Exception as e:
            error_handler.handle_error(e, "ValueAlignment", "Ethical review determination failed")
            return False
    
    def _integrate_assessments(self, value_assessment, ethical_assessment):
        """Integrate evaluation results"""
        try:
            # Validate input types
            if not isinstance(value_assessment, dict):
                logger.warning("_integrate_assessments: Invalid value_assessment format")
                return {
                    'alignment_score': 0.0,
                    'confidence': 0.5,
                    'primary_concerns': [],
                    'positive_aspects': [],
                    'requires_human_review': True
                }
            
            if ethical_assessment is None:
                # Only value evaluation with safety checks
                total_alignment_score = value_assessment.get('total_alignment_score', 0.0)
                violations = value_assessment.get('violations', [])
                fulfillments = value_assessment.get('fulfillments', [])
                
                # Ensure numeric type
                if not isinstance(total_alignment_score, (int, float)):
                    total_alignment_score = 0.0
                
                # Ensure list types to prevent 'int' object has no len() error
                if not hasattr(violations, '__iter__') or isinstance(violations, (str, dict)):
                    violations = []
                if not hasattr(fulfillments, '__iter__') or isinstance(fulfillments, (str, dict)):
                    fulfillments = []
                
                return {
                    'alignment_score': total_alignment_score,
                    'confidence': 0.8,
                    'primary_concerns': list(violations),
                    'positive_aspects': list(fulfillments)
                }
            
            # Validate ethical_assessment is a dictionary
            if not isinstance(ethical_assessment, dict):
                logger.warning("_integrate_assessments: Invalid ethical_assessment format")
                return {
                    'alignment_score': value_assessment.get('total_alignment_score', 0.0),
                    'confidence': 0.6,
                    'primary_concerns': list(value_assessment.get('violations', [])),
                    'positive_aspects': list(value_assessment.get('fulfillments', [])),
                    'requires_human_review': True
                }
            
            # Integrate value and ethical evaluation with safety checks
            # Get and validate value score
            value_score = value_assessment.get('total_alignment_score', 0.0)
            if not isinstance(value_score, (int, float)):
                value_score = 0.0
            
            # Get and validate ethical confidence
            ethical_confidence = ethical_assessment.get('confidence', 0.0)
            if not isinstance(ethical_confidence, (int, float)):
                ethical_confidence = 0.0
            
            # Get and validate ethical recommendation
            ethical_recommendation = 0.0
            try:
                consensus_rec = ethical_assessment.get('consensus_recommendation', {})
                if isinstance(consensus_rec, dict):
                    recommendation = consensus_rec.get('recommendation', False)
                    if isinstance(recommendation, bool):
                        ethical_recommendation = 1.0 if recommendation else 0.0
            except Exception as e:
                logger.warning(f"Failed to get ethical recommendation: {str(e)}")
            
            # Calculate integrated score
            integrated_score = (value_score * 0.6 + ethical_recommendation * 0.4) * ethical_confidence
            
            # Process concerns with safety checks
            concerns = []
            try:
                violations = value_assessment.get('violations', [])
                if hasattr(violations, '__iter__') and not isinstance(violations, (str, dict)):
                    concerns = list(violations)
                
                # Add ethical concerns if needed
                if ethical_recommendation < 0.5:
                    concerns.append('ethical_concerns')
            except Exception as e:
                logger.warning(f"Failed to process concerns: {str(e)}")
                concerns = []
            
            # Process positives with safety checks
            positives = []
            try:
                fulfillments = value_assessment.get('fulfillments', [])
                if hasattr(fulfillments, '__iter__') and not isinstance(fulfillments, (str, dict)):
                    positives = list(fulfillments)
                
                # Add ethical soundness if recommended
                if ethical_recommendation >= 0.5:
                    positives.append('ethically_sound')
            except Exception as e:
                logger.warning(f"Failed to process positives: {str(e)}")
                positives = []
            
            return {
                'alignment_score': integrated_score,
                'confidence': min(ethical_confidence, 0.9),  # Conservative confidence
                'primary_concerns': concerns,
                'positive_aspects': positives,
                'requires_human_review': integrated_score < 0.6
            }
        except Exception as e:
            error_handler.handle_error(e, "ValueAlignment", "Assessment integration failed")
            return {
                'alignment_score': 0.0,
                'confidence': 0.3,
                'primary_concerns': ['integration_failed'],
                'positive_aspects': [],
                'requires_human_review': True
            }
    
    def _make_verdict(self, assessment):
        """Make final verdict"""
        try:
            # Validate assessment is a dictionary
            if not isinstance(assessment, dict):
                logger.warning("_make_verdict: Invalid assessment format")
                return {
                    'verdict': 'REQUIRES_MODIFICATION',
                    'confidence': 'LOW',
                    'reasoning': 'Invalid assessment data'
                }
            
            # Safely get and validate alignment score
            score = assessment.get('alignment_score', 0.0)
            if not isinstance(score, (int, float)):
                try:
                    # Attempt to convert to float if possible
                    score = float(score)
                except (ValueError, TypeError):
                    logger.warning("_make_verdict: Invalid alignment score")
                    score = 0.0
            
            # Determine verdict based on score
            if score >= 0.8:
                return {
                    'verdict': 'APPROVED',
                    'confidence': 'HIGH',
                    'reasoning': 'Action highly aligns with value and ethical standards'
                }
            elif score >= 0.6:
                return {
                    'verdict': 'CONDITIONALLY_APPROVED',
                    'confidence': 'MEDIUM',
                    'reasoning': 'Action basically meets requirements, monitoring during execution recommended'
                }
            elif score >= 0.4:
                return {
                    'verdict': 'REQUIRES_MODIFICATION',
                    'confidence': 'LOW',
                    'reasoning': 'Action needs modification to meet value standards'
                }
            else:
                return {
                    'verdict': 'REJECTED',
                    'confidence': 'HIGH',
                    'reasoning': 'Action has serious value or ethical issues'
                }
        except Exception as e:
            error_handler.handle_error(e, "ValueAlignment", "Verdict generation failed")
            return {
                'verdict': 'REJECTED',
                'confidence': 'LOW',
                'reasoning': 'Error in verdict generation'
            }
    
    def get_alignment_report(self):
        """Get alignment report"""
        return {
            'value_system': self.value_system.get_value_statistics(),
            'ethical_reasoning': self.ethical_reasoner.get_ethical_report(),
            'overall_alignment_health': self._calculate_alignment_health(),
            'report_timestamp': datetime.now().isoformat()
        }
    
    def _calculate_alignment_health(self):
        """Calculate alignment health"""
        try:
            value_stats = self.value_system.get_value_statistics()
            ethical_report = self.ethical_reasoner.get_ethical_report()
            
            # Validate value_stats and ethical_report are dictionaries
            if not isinstance(value_stats, dict) or not isinstance(ethical_report, dict):
                logger.warning("_calculate_alignment_health: Invalid statistics format")
                return {'score': 0.0, 'status': 'UNKNOWN'}
            
            # Safely get violation and fulfillment counts
            total_violations = value_stats.get('total_violations', 0)
            total_fulfillments = value_stats.get('total_fulfillments', 1)
            cases_resolved = ethical_report.get('cases_resolved', 0)
            
            # Ensure numeric types
            if not isinstance(total_violations, (int, float)):
                total_violations = 0
            if not isinstance(total_fulfillments, (int, float)):
                total_fulfillments = 1
            if not isinstance(cases_resolved, (int, float)):
                cases_resolved = 0
            
            # Calculate ratios with safety checks
            violation_ratio = total_violations / max(1, total_fulfillments)
            # Use cases_resolved as numerator, but need to find a valid denominator
            case_success_ratio = cases_resolved / max(1, cases_resolved)
            
            health_score = max(0.0, min(1.0, 1.0 - violation_ratio * 0.5 + case_success_ratio * 0.3))
            
            if health_score > 0.8:
                return {'score': health_score, 'status': 'EXCELLENT'}
            elif health_score > 0.6:
                return {'score': health_score, 'status': 'GOOD'}
            elif health_score > 0.4:
                return {'score': health_score, 'status': 'FAIR'}
            else:
                return {'score': health_score, 'status': 'POOR'}
        except Exception as e:
            error_handler.handle_error(e, "ValueAlignment", "Alignment health calculation failed")
            return {'score': 0.0, 'status': 'ERROR'}
    
    def export_alignment_data(self, file_path):
        """Export alignment data"""
        try:
            # Validate file_path is a string
            if not isinstance(file_path, str):
                raise TypeError("File path must be a string")
            
            # Safely get value statistics
            value_statistics = {}
            try:
                value_statistics = self.value_system.get_value_statistics()
                if not isinstance(value_statistics, dict):
                    value_statistics = {}
            except Exception as e:
                logger.warning(f"Failed to get value statistics: {str(e)}")
            
            # Safely get ethical cases with type checks to prevent 'int' object has no len() error
            ethical_cases = []
            try:
                # Check if ethical_reasoner has case_studies attribute and it's iterable
                if hasattr(self.ethical_reasoner, 'case_studies'):
                    if hasattr(self.ethical_reasoner.case_studies, '__iter__'):
                        # Convert to list and get last 20 cases
                        ethical_cases = list(self.ethical_reasoner.case_studies)[-20:]
                    else:
                        logger.warning("case_studies is not iterable")
                else:
                    logger.warning("No case_studies attribute found")
            except Exception as e:
                logger.warning(f"Failed to get ethical cases: {str(e)}")
            
            # Safely calculate alignment health
            alignment_health = self._calculate_alignment_health()
            
            # Prepare export data
            export_data = {
                'value_statistics': value_statistics,
                'ethical_cases': ethical_cases,
                'alignment_health': alignment_health,
                'export_timestamp': datetime.now().isoformat()
            }
            
            # Write to file
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            return {"success": True, "export_path": file_path}
            
        except Exception as e:
            error_handler.handle_error(e, "ValueAlignment", "Alignment data export failed")
            return {"error": str(e)}
